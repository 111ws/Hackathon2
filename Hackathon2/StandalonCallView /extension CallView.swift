//
//  Untitled.swift
//  Aura calling
//
//  Created by Macbook pro on 2025/8/17.
//
import SwiftUI
import AVFoundation
import Speech


extension StandaloneCallView {
    func addDebugMessage(_ message: String) {
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
        let debugMessage = "[\(timestamp)] \(message)"
        debugMessages.append(debugMessage)
        
        if debugMessages.count > 50 {
            debugMessages.removeFirst()
        }
        
        print(debugMessage)
    }
    
    func startCallTimer() {
        timer?.invalidate()
        callDuration = 0
        timer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { _ in
            self.callDuration += 1
        }
    }
    
    func stopCallTimer() {
        timer?.invalidate()
        timer = nil
    }
    
    func requestSpeechAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self.addDebugMessage("Speech recognition authorized")
                    self.startContinuousSpeechRecognition()
                case .denied:
                    self.addDebugMessage("Speech recognition denied")
                case .restricted:
                    self.addDebugMessage("Speech recognition restricted")
                case .notDetermined:
                    self.addDebugMessage("Speech recognition not determined")
                @unknown default:
                    self.addDebugMessage("Speech recognition unknown status")
                }
            }
        }
    }
    
    func setupAudioSession() {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // ä¼˜åŒ–éŸ³é¢‘ä¼šè¯é…ç½®ä»¥æ›´å¥½åœ°æ”¯æŒMP3æ’­æ”¾
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.allowBluetooth, .defaultToSpeaker, .duckOthers])
            try audioSession.setActive(true)
            addDebugMessage("âœ… Audio session configured for MP3 playback")
        } catch {
            addDebugMessage("âŒ Audio session setup failed: \(error.localizedDescription)")
        }
    }
    func audioBufferToData(_ buffer: AVAudioPCMBuffer) -> Data {
        let channelData = buffer.floatChannelData![0]
        let frameLength = Int(buffer.frameLength)
        
        let data = Data(bytes: channelData, count: frameLength * MemoryLayout<Float>.size)
        return data
    }
    
    // åˆ é™¤è¿™äº›å­˜å‚¨å±æ€§å®šä¹‰ï¼ˆç¬¬78-88è¡Œï¼‰ï¼š
    // private var isRecording = false
    // private var speechBuffer = Data()
    // private var silenceTimer: Timer?
    // private var lastSpeechTime: Date?
    // private var speechStartTime: Date?
    // private let silenceThreshold: Float = -40.0
    // private let silenceDuration: TimeInterval = 1.5
    // private let minSpeechDuration: TimeInterval = 0.5
    // private let maxSpeechDuration: TimeInterval = 10.0
    
    func startContinuousSpeechRecognition() {
        guard !isMuted else { return }
        
        stopSpeechRecognition()
        
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.allowBluetooth, .defaultToSpeaker, .duckOthers])
            try audioSession.setActive(true)
        } catch {
            addDebugMessage("Audio session configuration failed: \(error.localizedDescription)")
            return
        }
        
        let node = audioEngine.inputNode
        let recordingFormat = node.outputFormat(forBus: 0)
        
        // é‡ç½®VADçŠ¶æ€
        resetVADState()
        
        node.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, time in
            guard let self = self else { return }
            
            // è®¡ç®—éŸ³é¢‘éŸ³é‡
            let volume = self.calculateVolume(from: buffer)
            
            // è¯­éŸ³æ´»åŠ¨æ£€æµ‹
            self.processVoiceActivity(buffer: buffer, volume: volume)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
            addDebugMessage("ğŸ¤ Voice Activity Detection started")
        } catch {
            addDebugMessage("âŒ Audio recording startup failed: \(error.localizedDescription)")
        }
    }
    
    // MARK: - è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ ¸å¿ƒé€»è¾‘
    
    private func processVoiceActivity(buffer: AVAudioPCMBuffer, volume: Float) {
        let currentTime = Date()
        let isSpeech = volume > silenceThreshold
        
        if isSpeech {
            // æ£€æµ‹åˆ°è¯­éŸ³
            handleSpeechDetected(buffer: buffer, currentTime: currentTime)
        } else {
            // æ£€æµ‹åˆ°é™éŸ³
            handleSilenceDetected(currentTime: currentTime)
        }
        
        // æ£€æŸ¥æœ€å¤§å½•éŸ³æ—¶é•¿
        checkMaxRecordingDuration(currentTime: currentTime)
    }
    
    private func handleSpeechDetected(buffer: AVAudioPCMBuffer, currentTime: Date) {
        lastSpeechTime = currentTime
        
        // å–æ¶ˆé™éŸ³è®¡æ—¶å™¨
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        if !isRecording {
            // å¼€å§‹å½•éŸ³
            startRecording(currentTime: currentTime)
        }
        
        // æ·»åŠ éŸ³é¢‘æ•°æ®åˆ°ç¼“å†²åŒº
        let audioData = audioBufferToData(buffer)
        speechBuffer.append(audioData)
        
        DispatchQueue.main.async {
            self.addDebugMessage("ğŸ—£ï¸ Speech detected, buffer size: \(self.speechBuffer.count) bytes")
        }
    }
    
    private func handleSilenceDetected(currentTime: Date) {
        if isRecording && silenceTimer == nil {
            // å¼€å§‹é™éŸ³è®¡æ—¶
            silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceDuration, repeats: false) { [weak self] _ in
                self?.finishRecording(reason: "Silence detected")
            }
            
            DispatchQueue.main.async {
                self.addDebugMessage("ğŸ¤« Silence detected, starting timer...")
            }
        }
    }
    
    private func startRecording(currentTime: Date) {
        isRecording = true
        speechStartTime = currentTime
        speechBuffer.removeAll()
        
        DispatchQueue.main.async {
            self.addDebugMessage("ğŸ™ï¸ Recording started")
        }
    }
    
    private func finishRecording(reason: String) {
        guard isRecording else { return }
        
        let recordingDuration = speechStartTime.map { Date().timeIntervalSince($0) } ?? 0
        
        // æ£€æŸ¥æœ€å°å½•éŸ³æ—¶é•¿
        if recordingDuration < minSpeechDuration {
            DispatchQueue.main.async {
                self.addDebugMessage("âš ï¸ Recording too short (\(String(format: "%.1f", recordingDuration))s), discarded")
            }
            resetVADState()
            return
        }
        
        // å‘é€éŸ³é¢‘æ•°æ®
        let finalBuffer = speechBuffer
        
        DispatchQueue.main.async {
            self.addDebugMessage("âœ… Recording finished (\(reason)): \(String(format: "%.1f", recordingDuration))s, \(finalBuffer.count) bytes")
            self.sendAudioBufferToBackend(finalBuffer)
        }
        
        resetVADState()
    }
    
    private func checkMaxRecordingDuration(currentTime: Date) {
        guard isRecording,
              let startTime = speechStartTime,
              currentTime.timeIntervalSince(startTime) > maxSpeechDuration else { return }
        
        finishRecording(reason: "Max duration reached")
    }
    
    private func resetVADState() {
        isRecording = false
        speechBuffer.removeAll()
        silenceTimer?.invalidate()
        silenceTimer = nil
        lastSpeechTime = nil
        speechStartTime = nil
    }
    
    // MARK: - éŸ³é‡è®¡ç®—
    
    private func calculateVolume(from buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return -100.0 }
        
        let frameLength = Int(buffer.frameLength)
        var sum: Float = 0.0
        
        // è®¡ç®—RMS (Root Mean Square)
        for i in 0..<frameLength {
            let sample = channelData[i]
            sum += sample * sample
        }
        
        let rms = sqrt(sum / Float(frameLength))
        
        // è½¬æ¢ä¸ºåˆ†è´
        let db = 20.0 * log10(max(rms, 1e-10))
        return db
    }
    
    // MARK: - åœæ­¢å½•éŸ³
    
    // ä¿ç•™ç¬¬256è¡Œçš„stopSpeechRecognitionå‡½æ•°ï¼Œåˆ é™¤ç¬¬495è¡Œçš„é‡å¤å®šä¹‰
    func stopSpeechRecognition() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œå®Œæˆå½“å‰å½•éŸ³
        if isRecording {
            finishRecording(reason: "Manual stop")
        }
        
        resetVADState()
        addDebugMessage("ğŸ›‘ Voice Activity Detection stopped")
    }

    
    
    func createWAVData(from pcmData: Data, sampleRate: Double) -> Data {
        let wavHeaderSize = 44
        let totalAudioLen = pcmData.count
        let totalDataLen = totalAudioLen + wavHeaderSize - 8
        let byteRate = Int(sampleRate) * 2 * 1 // å‡è®¾16bit, mono
        
        var wavHeader = Data()
        
        // RIFF header
        wavHeader.append("RIFF".data(using: .ascii)!)
        wavHeader.append(contentsOf: withUnsafeBytes(of: totalDataLen.littleEndian) { Data($0) })
        wavHeader.append("WAVE".data(using: .ascii)!)
        
        // fmt chunk
        wavHeader.append("fmt ".data(using: .ascii)!)
        wavHeader.append(contentsOf: withUnsafeBytes(of: 16.littleEndian) { Data($0) }) // Subchunk1Size
        wavHeader.append(contentsOf: withUnsafeBytes(of: 1.littleEndian) { Data($0) }) // AudioFormat (PCM)
        wavHeader.append(contentsOf: withUnsafeBytes(of: 1.littleEndian) { Data($0) }) // NumChannels (Mono)
        wavHeader.append(contentsOf: withUnsafeBytes(of: Int32(sampleRate).littleEndian) { Data($0) })
        wavHeader.append(contentsOf: withUnsafeBytes(of: Int32(byteRate).littleEndian) { Data($0) })
        wavHeader.append(contentsOf: withUnsafeBytes(of: 2.littleEndian) { Data($0) }) // BlockAlign
        wavHeader.append(contentsOf: withUnsafeBytes(of: 16.littleEndian) { Data($0) }) // BitsPerSample
        
        // data chunk
        wavHeader.append("data".data(using: .ascii)!)
        wavHeader.append(contentsOf: withUnsafeBytes(of: totalAudioLen.littleEndian) { Data($0) })
        
        return wavHeader + pcmData
    }
    func sendAudioBufferToBackend(_ audioData: Data) {
        guard !audioData.isEmpty else { return }
            
            addDebugMessage("ğŸ“¤ Sending audio data to backendï¼ˆå‘é€éŸ³é¢‘åˆ°åå°ï¼‰: \(audioData.count) bytes")
            
            let url = URL(string: "https://emohunter-api-6106408799.us-central1.run.app/api/v1/voice_conversation")!
            var request = URLRequest(url: url)
            request.httpMethod = "POST"
            request.timeoutInterval = 30.0
            
            // åˆ›å»ºmultipart/form-dataæ ¼å¼
            let boundary = "Boundary-\(UUID().uuidString)"
            request.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")
            
            // æ„å»ºmultipart body
            var body = Data()
            
            // æ·»åŠ éŸ³é¢‘æ–‡ä»¶
            body.append("--\(boundary)\r\n".data(using: .utf8)!)
            body.append("Content-Disposition: form-data; name=\"audio\"; filename=\"audio.wav\"\r\n".data(using: .utf8)!)
            body.append("Content-Type: audio/wav\r\n\r\n".data(using: .utf8)!)
            body.append(audioData)
            body.append("\r\n".data(using: .utf8)!)
            
            // æ·»åŠ user_idå­—æ®µ
            body.append("--\(boundary)\r\n".data(using: .utf8)!)
            body.append("Content-Disposition: form-data; name=\"user_id\"\r\n\r\n".data(using: .utf8)!)
            body.append(userId.data(using: .utf8)!)
            body.append("\r\n".data(using: .utf8)!)
            
            // ç»“æŸboundary
            body.append("--\(boundary)--\r\n".data(using: .utf8)!)
            
            request.httpBody = body
            addDebugMessage("âœ… Audio data encoded as multipart/form-data")
            
            // ç½‘ç»œè¯·æ±‚æ‰§è¡Œä»£ç ä¿æŒä¸å˜...
            let startTime = Date()
            // åœ¨ sendAudioBufferToBackend å‡½æ•°ä¸­ï¼Œæ›¿æ¢ç°æœ‰çš„å“åº”å¤„ç†ä»£ç 
            URLSession.shared.dataTask(with: request) { data, response, error in
                let responseTime = Date().timeIntervalSince(startTime)
                
                DispatchQueue.main.async {
                    if let error = error {
                        self.addDebugMessage("âŒ API request failed (\(String(format: "%.2f", responseTime))s): \(error.localizedDescription)")
                        return
                    }
                    
                    if let httpResponse = response as? HTTPURLResponse {
                        self.addDebugMessage("âœ… API response status code: \(httpResponse.statusCode) (duration: \(String(format: "%.2f", responseTime)) seconds)")
                        
                        // æ·»åŠ å“åº”å¤´ä¿¡æ¯è°ƒè¯•
                        if let contentType = httpResponse.value(forHTTPHeaderField: "Content-Type") {
                            self.addDebugMessage("ğŸ“‹ Response Content-Type: \(contentType)")
                        }
                        if let contentLength = httpResponse.value(forHTTPHeaderField: "Content-Length") {
                            self.addDebugMessage("ğŸ“ Response Content-Length: \(contentLength)")
                        }
                    }
                    
                    // æ”¹è¿›æ•°æ®å¤„ç†é€»è¾‘
                    if let data = data {
                        self.addDebugMessage("ğŸ“¥ Received \(data.count) bytes of response data")
                        
                        if data.count == 0 {
                            self.addDebugMessage("âš ï¸ Response data is empty")
                            return
                        }
                        
                        if let responseString = String(data: data, encoding: .utf8) {
                            self.addDebugMessage("ğŸ“¥ Response content: \(responseString.prefix(500))...")
                            
                            do {
                                if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                                    self.addDebugMessage("âœ… Successfully parsed JSON response")
                                    
                                    if let audioUrl = json["audio_url"] as? String {
                                        self.addDebugMessage("ğŸµ Received MP3 audio URL: \(audioUrl)")
                                        self.playMP3AudioFromURL(audioUrl)
                                    } else if let audioData = json["audio_data"] as? String {
                                        self.addDebugMessage("ğŸµ Received base64 audio data (\(audioData.count) characters)")
                                        self.playMP3AudioFromBase64(audioData)
                                    } else {
                                        self.addDebugMessage("âŒ No audio_url or audio_data found in response")
                                        self.addDebugMessage("ğŸ“‹ Available keys: \(Array(json.keys))")
                                    }
                                } else {
                                    self.addDebugMessage("âŒ Response is not valid JSON")
                                }
                            } catch {
                                self.addDebugMessage("âŒ JSON parsing failed: \(error.localizedDescription)")
                                // å°è¯•ç›´æ¥æ’­æ”¾éŸ³é¢‘æ•°æ®
                                self.addDebugMessage("ğŸ”„ Attempting to play response as direct audio data")
                                self.playMP3AudioData(data)
                            }
                        } else {
                            self.addDebugMessage("âŒ Cannot convert response data to string, trying as binary audio")
                            // å°è¯•ç›´æ¥æ’­æ”¾äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
                            self.playMP3AudioData(data)
                        }
                    } else {
                        self.addDebugMessage("âŒ No response data received (data is nil)")
                    }
                }
            }
            .resume()
    }
    
    func playMP3AudioFromURL(_ urlString: String) {
        guard let url = URL(string: urlString) else {
            addDebugMessage("âŒ Invalid MP3 audio URL")
            return
        }
        
        stopSpeechRecognition()
        addDebugMessage("ğŸµ Starting MP3 download from: \(urlString)")
        
        URLSession.shared.dataTask(with: url) { data, response, error in
            DispatchQueue.main.async {
                if let error = error {
                    self.addDebugMessage("âŒ MP3 audio download failed: \(error.localizedDescription)")
                    return
                }
                
                guard let data = data else {
                    self.addDebugMessage("âŒ MP3 audio data is empty")
                    return
                }
                
                self.addDebugMessage("âœ… MP3 downloaded successfully (\(data.count) bytes)")
                self.playMP3AudioData(data)
            }
        }.resume()
    }
    
    func playMP3AudioFromBase64(_ base64String: String) {
        guard let data = Data(base64Encoded: base64String) else {
            addDebugMessage("âŒ Invalid base64 audio data")
            return
        }
        
        stopSpeechRecognition()
        addDebugMessage("ğŸµ Playing MP3 from base64 data (\(data.count) bytes)")
        playMP3AudioData(data)
    }
    
    func playMP3AudioData(_ data: Data) {
        do {
            // åœæ­¢ä¹‹å‰çš„éŸ³é¢‘æ’­æ”¾
            stopAudioPlayback()
            
            // åˆ›å»ºæ–°çš„éŸ³é¢‘æ’­æ”¾å™¨
            self.audioPlayer = try AVAudioPlayer(data: data)
            self.audioPlayer?.delegate = AudioPlayerDelegate(parent: self)
            self.audioPlayer?.prepareToPlay()
            
            self.currentAudioDuration = self.audioPlayer?.duration ?? 0
            self.isPlayingResponse = true
            
            // å¼€å§‹æ’­æ”¾
            self.audioPlayer?.play()
            self.startPlaybackProgressTimer()
            
            // Fix the string interpolation syntax error
            self.addDebugMessage("ğŸµ Started playing MP3 audio (duration: \(String(format: "%.1f", self.currentAudioDuration))s)")
        } catch {
            self.addDebugMessage("âŒ MP3 audio playback error: \(error.localizedDescription)")
            self.isPlayingResponse = false
            self.restartSpeechRecognition()
        }
    }
    
    // æ³¨é‡Šæ‰æˆ–åˆ é™¤åŸæ¥çš„speakTextæ–¹æ³•ï¼Œå› ä¸ºä¸å†ä½¿ç”¨AVSpeechSynthesizer
    /*
    func speakText(_ text: String) {
        // åŸæ¥çš„AVSpeechSynthesizerä»£ç 
    }
    */
    
    // ç®€åŒ–requestTTSAudioæ–¹æ³•
    func requestTTSAudio(text: String) {
        addDebugMessage(" Text response received, but using MP3 audio instead: \(text)")
        // ä¸å†éœ€è¦TTSï¼Œå› ä¸ºAPIç›´æ¥è¿”å›MP3éŸ³é¢‘
    }
    
    func startPlaybackProgressTimer() {
        playbackTimer?.invalidate()
        playbackTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [self] _ in
            guard let player = self.audioPlayer else { return }
            
            let currentTime = player.currentTime
            let progress = Float(currentTime / player.duration)
            self.audioPlaybackProgress = progress
        }
    }
    
    
    
    func restartSpeechRecognition() {
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            if self.isCallActive && !self.isMuted {
                self.startContinuousSpeechRecognition()
            }
        }
    }
    
    func stopAudioPlayback() {
        audioPlayer?.stop()
        audioPlayer = nil
        isPlayingResponse = false
        stopPlaybackProgressTimer()
    }
    
    func stopPlaybackProgressTimer() {
        playbackTimer?.invalidate()
        playbackTimer = nil
    }
    
    func endCall() {
        isCallActive = false
        stopCallTimer()
        stopSpeechRecognition()
        stopAudioPlayback()
    }
    
    func timeString(from timeInterval: TimeInterval) -> String {
        let minutes = Int(timeInterval) / 60
        let seconds = Int(timeInterval) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
}
