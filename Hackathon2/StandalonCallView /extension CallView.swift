//
//  Untitled.swift
//  Aura calling
//
//  Created by Macbook pro on 2025/8/17.
//
import SwiftUI
import AVFoundation
import Speech


extension StandaloneCallView {
    func addDebugMessage(_ message: String) {
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
        let debugMessage = "[\(timestamp)] \(message)"
        debugMessages.append(debugMessage)
        
        if debugMessages.count > 50 {
            debugMessages.removeFirst()
        }
        
        print(debugMessage)
    }
    
    func startCallTimer() {
        timer?.invalidate()
        callDuration = 0
        timer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { _ in
            self.callDuration += 1
        }
    }
    
    func stopCallTimer() {
        timer?.invalidate()
        timer = nil
    }
    
    func requestSpeechAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self.addDebugMessage("Speech recognition authorized")
                    self.startContinuousSpeechRecognition()
                case .denied:
                    self.addDebugMessage("Speech recognition denied")
                case .restricted:
                    self.addDebugMessage("Speech recognition restricted")
                case .notDetermined:
                    self.addDebugMessage("Speech recognition not determined")
                @unknown default:
                    self.addDebugMessage("Speech recognition unknown status")
                }
            }
        }
    }
    
    func setupAudioSession() {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // ä¼˜åŒ–éŸ³é¢‘ä¼šè¯é…ç½®ä»¥æ›´å¥½åœ°æ”¯æŒMP3æ’­æ”¾
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.allowBluetooth, .defaultToSpeaker, .duckOthers])
            try audioSession.setActive(true)
            addDebugMessage("âœ… Audio session configured for MP3 playback")
        } catch {
            addDebugMessage("âŒ Audio session setup failed: \(error.localizedDescription)")
        }
    }
    func audioBufferToData(_ buffer: AVAudioPCMBuffer) -> Data {
        let channelData = buffer.floatChannelData![0]
        let frameLength = Int(buffer.frameLength)
        
        let data = Data(bytes: channelData, count: frameLength * MemoryLayout<Float>.size)
        return data
    }
    
    // åˆ é™¤è¿™äº›å­˜å‚¨å±žæ€§å®šä¹‰ï¼ˆç¬¬78-88è¡Œï¼‰ï¼š
    // private var isRecording = false
    // private var speechBuffer = Data()
    // private var silenceTimer: Timer?
    // private var lastSpeechTime: Date?
    // private var speechStartTime: Date?
    // private let silenceThreshold: Float = -40.0
    // private let silenceDuration: TimeInterval = 1.5
    // private let minSpeechDuration: TimeInterval = 0.5
    // private let maxSpeechDuration: TimeInterval = 10.0
    
    
    func startContinuousSpeechRecognition() {
        guard !isMuted,
              speechRecognizer?.isAvailable == true,
              SFSpeechRecognizer.authorizationStatus() == .authorized else {
            return
        }
        
        stopSpeechRecognition()
        
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.allowBluetooth, .defaultToSpeaker, .duckOthers])
            try audioSession.setActive(true)
        } catch {
            addDebugMessage("Audio session configuration failed: \(error.localizedDescription)")
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.requiresOnDeviceRecognition = false
        
        // åœ¨startContinuousSpeechRecognitionæ–¹æ³•ä¸­ï¼Œæ›¿æ¢recognitionTaskçš„å¤„ç†é€»è¾‘
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
        DispatchQueue.main.async {
        if let result = result {
        let transcription = result.bestTranscription.formattedString
        
        // æ›´æ–°å½“å‰è¯­éŸ³æ–‡æœ¬æ˜¾ç¤º
        self.currentSpeechText = transcription
        self.isSpeaking = !result.isFinal
        
        // å¦‚æžœæ–‡æœ¬å‘ç”Ÿå˜åŒ–ä¸”ä¸ä¸ºç©ºï¼Œç«‹å³å‘é€
        if !transcription.isEmpty && transcription != self.previousTranscription {
        self.previousTranscription = transcription
        
        // å–æ¶ˆä¹‹å‰çš„å®šæ—¶å™¨
        self.speechTimer?.invalidate()
        
        // è®¾ç½®æ–°çš„å®šæ—¶å™¨ï¼Œå»¶è¿Ÿå‘é€ä»¥ç¡®ä¿è¯­éŸ³è¯†åˆ«å®Œæˆ
        self.speechTimer = Timer.scheduledTimer(withTimeInterval: 1.5, repeats: false) { _ in
        if !transcription.isEmpty {
        self.sendSpeechToBackend(message: transcription)
        self.speechHistory.append(transcription)
        self.currentSpeechText = ""
        self.previousTranscription = ""
        }
        }
        }
        
        // å¦‚æžœæ˜¯æœ€ç»ˆç»“æžœï¼Œç«‹å³å‘é€
        if result.isFinal {
        self.speechTimer?.invalidate()
        if !transcription.isEmpty {
        self.sendSpeechToBackend(message: transcription)
        self.speechHistory.append(transcription)
        self.currentSpeechText = ""
        self.previousTranscription = ""
        }
        }
        }
        
        if error != nil {
        self.addDebugMessage("âŒ Speech recognition error: \(error?.localizedDescription ?? "Unknown error")")
        self.restartSpeechRecognition()
        }
        }
        }
        
        let node = audioEngine.inputNode
        let recordingFormat = node.outputFormat(forBus: 0)
        
        node.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
            self.addDebugMessage("Speech recognition started")
        } catch {
            self.addDebugMessage("Speech recognition startup failed: \(error.localizedDescription)")
        }
    }
    
    func speakText(_ text: String) {
        // åœæ­¢å½“å‰è¯­éŸ³è¯†åˆ«ï¼Œé¿å…å†²çª
        stopSpeechRecognition()
        
        // åœæ­¢å½“å‰æ­£åœ¨æ’­æ”¾çš„è¯­éŸ³
        speechSynthesizer.stopSpeaking(at: .immediate)
        
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US") // æˆ–è€…ä½¿ç”¨"zh-CN"ä¸­æ–‡
        utterance.rate = 0.5 // è¯­é€Ÿï¼ŒèŒƒå›´0.0-1.0
        utterance.pitchMultiplier = 1.0 // éŸ³è°ƒ
        utterance.volume = 1.0 // éŸ³é‡
        
        // è®¾ç½®æ’­æ”¾çŠ¶æ€
        isPlayingResponse = true
        addDebugMessage("ðŸ”Š Started speaking AI response")
        
        speechSynthesizer.speak(utterance)
        
        // ç›‘å¬è¯­éŸ³åˆæˆå®Œæˆ
        DispatchQueue.main.asyncAfter(deadline: .now() + Double(text.count) * 0.1) {
            self.isPlayingResponse = false
            self.addDebugMessage("ï¿½ Finished speaking AI response")
            self.restartSpeechRecognition()
        }
    }
    
    func sendSpeechToBackend(message: String) {
        guard !message.isEmpty else { return }
        
        addDebugMessage("ðŸ“¤ Sending speech to backend: \"\(message.prefix(20))...\"")
        
        // æ›´æ”¹ä¸ºæ–°çš„APIæŽ¥å£
        let url = URL(string: "https://emohunter-api-6106408799.us-central1.run.app/api/v1/text_conversation")!
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.timeoutInterval = 30.0
        
        let requestBody = [
            "message": message,
            "user_id": userId
        ]
        
        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
            addDebugMessage("âœ… Request body serialized successfully")
        } catch {
            addDebugMessage("âŒ JSON serialization failed: \(error.localizedDescription)")
            return
        }
        
        let startTime = Date()
        URLSession.shared.dataTask(with: request) { data, response, error in
            let responseTime = Date().timeIntervalSince(startTime)
            
            DispatchQueue.main.async {
                if let error = error {
                    self.addDebugMessage("âŒ API request failed (\(String(format: "%.2f", responseTime))s): \(error.localizedDescription)")
                    return
                }
                
                if let httpResponse = response as? HTTPURLResponse {
                    self.addDebugMessage("âœ… API response status code: \(httpResponse.statusCode)")
                }
                
                if let data = data, let responseString = String(data: data, encoding: .utf8) {
                    self.addDebugMessage("ðŸ“¥ Received response data: \(responseString.prefix(200))...")
                    
                    do {
                        if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                            // æ–°APIç›´æŽ¥è¿”å›žéŸ³é¢‘URLï¼Œä¸å†ä½¿ç”¨AVSpeechSynthesizer
                            if let audioUrl = json["audio_url"] as? String {
                                self.addDebugMessage("ðŸŽµ Received MP3 audio URL: \(audioUrl)")
                                self.playMP3AudioFromURL(audioUrl)
                            } else if let audioData = json["audio_data"] as? String {
                                // å¦‚æžœè¿”å›žbase64ç¼–ç çš„éŸ³é¢‘æ•°æ®
                                self.addDebugMessage("ðŸŽµ Received base64 audio data")
                                self.playMP3AudioFromBase64(audioData)
                            } else {
                                self.addDebugMessage("âŒ No audio data found in response")
                            }
                        }
                    } catch {
                        self.addDebugMessage("âŒ Response parsing failed: \(error.localizedDescription)")
                    }
                }
            }
        }.resume()
    }
   
    func stopSpeechRecognition() {
            speechTimer?.invalidate()
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
            recognitionRequest?.endAudio()
            recognitionTask?.cancel()
    
            recognitionRequest = nil
            recognitionTask = nil
            currentSpeechText = ""
            isSpeaking = false
        }
    
        func restartSpeechRecognition() {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                if self.isCallActive && !self.isMuted {
                    self.startContinuousSpeechRecognition()
                }
            }
        }
    func playMP3AudioFromURL(_ urlString: String) {
        guard let url = URL(string: urlString) else {
            addDebugMessage("âŒ Invalid MP3 audio URL")
            return
        }
        
        stopSpeechRecognition()
        addDebugMessage("ðŸŽµ Starting MP3 download from: \(urlString)")
        
        URLSession.shared.dataTask(with: url) { data, response, error in
            DispatchQueue.main.async {
                if let error = error {
                    self.addDebugMessage("âŒ MP3 audio download failed: \(error.localizedDescription)")
                    return
                }
                
                guard let data = data else {
                    self.addDebugMessage("âŒ MP3 audio data is empty")
                    return
                }
                
                self.addDebugMessage("âœ… MP3 downloaded successfully (\(data.count) bytes)")
                self.playMP3AudioData(data)
            }
        }.resume()
    }
    
    func playMP3AudioFromBase64(_ base64String: String) {
        guard let data = Data(base64Encoded: base64String) else {
            addDebugMessage("âŒ Invalid base64 audio data")
            return
        }
        
        stopSpeechRecognition()
        addDebugMessage("ðŸŽµ Playing MP3 from base64 data (\(data.count) bytes)")
        playMP3AudioData(data)
    }
    
    func playMP3AudioData(_ data: Data) {
        do {
            // åœæ­¢ä¹‹å‰çš„éŸ³é¢‘æ’­æ”¾
            stopAudioPlayback()
            
            // åˆ›å»ºæ–°çš„éŸ³é¢‘æ’­æ”¾å™¨
            self.audioPlayer = try AVAudioPlayer(data: data)
            self.audioPlayer?.delegate = AudioPlayerDelegate(parent: self)
            self.audioPlayer?.prepareToPlay()
            
            self.currentAudioDuration = self.audioPlayer?.duration ?? 0
            self.isPlayingResponse = true
            
            // å¼€å§‹æ’­æ”¾
            self.audioPlayer?.play()
            self.startPlaybackProgressTimer()
            
            // Fix the string interpolation syntax error
            self.addDebugMessage("ðŸŽµ Started playing MP3 audio (duration: \(String(format: "%.1f", self.currentAudioDuration))s)")
        } catch {
            self.addDebugMessage("âŒ MP3 audio playback error: \(error.localizedDescription)")
            self.isPlayingResponse = false
            self.restartSpeechRecognition()
        }
    }
    
    // æ³¨é‡ŠæŽ‰æˆ–åˆ é™¤åŽŸæ¥çš„speakTextæ–¹æ³•ï¼Œå› ä¸ºä¸å†ä½¿ç”¨AVSpeechSynthesizer
    /*
    func speakText(_ text: String) {
        // åŽŸæ¥çš„AVSpeechSynthesizerä»£ç 
    }
    */
    
    // ç®€åŒ–requestTTSAudioæ–¹æ³•
    func requestTTSAudio(text: String) {
        addDebugMessage(" Text response received, but using MP3 audio instead: \(text)")
        // ä¸å†éœ€è¦TTSï¼Œå› ä¸ºAPIç›´æŽ¥è¿”å›žMP3éŸ³é¢‘
    }
    
    func startPlaybackProgressTimer() {
        playbackTimer?.invalidate()
        playbackTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [self] _ in
            guard let player = self.audioPlayer else { return }
            
            let currentTime = player.currentTime
            let progress = Float(currentTime / player.duration)
            self.audioPlaybackProgress = progress
        }
    }
    
    
    
   
    
    func stopAudioPlayback() {
        audioPlayer?.stop()
        audioPlayer = nil
        isPlayingResponse = false
        stopPlaybackProgressTimer()
    }
    
    func stopPlaybackProgressTimer() {
        playbackTimer?.invalidate()
        playbackTimer = nil
    }
    
    func endCall() {
        isCallActive = false
        stopCallTimer()
        stopSpeechRecognition()
        stopAudioPlayback()
    }
    
    func timeString(from timeInterval: TimeInterval) -> String {
        let minutes = Int(timeInterval) / 60
        let seconds = Int(timeInterval) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
}
